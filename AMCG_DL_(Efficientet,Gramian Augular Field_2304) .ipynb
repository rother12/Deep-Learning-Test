{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For DL based on Gramian Angular Field in All Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pickle Path Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "import torch\n",
    "from typing import TypeVar, Tuple, List, Union, Dict, Callable, Optional\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Diagnosis With SpreadSheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "__Version__ = '0.0.2'\n",
    "__author__ = 'Won sik jung'\n",
    "'''S_Data Based on R/T Tangent images map'''\n",
    "\n",
    "origin_path = '/home/yongho.choi/Data/severance/processed/'\n",
    "\n",
    "\n",
    "Healthy = [str(origin_path)+x+'/PARAMETER4/' for x in os.listdir(origin_path) if 'Healthy' in x]\n",
    "patient_path = origin_path+\"patient/PARAMETER4/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraction Pickle path in Some Path\n",
    "# label= 0 or 1\n",
    "# 0 = 건강인(Healthy), 1 = 환자(Patient)\n",
    "\n",
    "def extrack_pickle(path,names=None,option=None):\n",
    "    result=list()\n",
    "    for path, dirs, files in os.walk(path):\n",
    "        for file in files:\n",
    "            name, ext = os.path.splitext(file)\n",
    "            \n",
    "            # if ext == '.pickle' and 'Images' in name:\n",
    "            if ext == '.pickle' and 'Images' in name:\n",
    "\n",
    "                # find folder\n",
    "                if names==None:\n",
    "                    # file_path = os.path.join(path,file)\n",
    "                    file_path = os.path.join(path,file)+'label='+str(option)\n",
    "\n",
    "                    result.append(file_path)\n",
    "\n",
    "                # find folder Contain names\n",
    "                elif str(names) in name:\n",
    "                    # file_path = os.path.join(path,file)\n",
    "                    file_path = os.path.join(path,file)+'label='+str(option)\n",
    "\n",
    "                    result.append(file_path)\n",
    "\n",
    "                else:\n",
    "                    pass\n",
    "\n",
    "    return sorted(result)\n",
    "\n",
    "#모든 건강인의 1번 bin 파일에 0label 추가\n",
    "\n",
    "Path_healthy_image=list()\n",
    "\n",
    "for path in Healthy:\n",
    "    Path_healthy_image.extend(extrack_pickle(path,'_1_','0'))\n",
    "\n",
    "#모든 환자군의 1번 bin 파일에 1label 추가\n",
    "Path_patient_image = extrack_pickle(patient_path,'_1_','1')\n",
    "\n",
    "#모든 1번 파일을 path_total_fieldmap에 적재\n",
    "path_total_image = Path_healthy_image + Path_patient_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1383"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(path_total_fieldmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "환자경로/home/yongho.choi/Data/severance/processed/patient/PARAMETER4/, 건강인 폴더['/home/yongho.choi/Data/severance/processed/Healthy_hospital/PARAMETER4/', '/home/yongho.choi/Data/severance/processed/Healthy_adenosin/PARAMETER4/']\n",
      "\n",
      "건강인(Healthy) = 137,환자(Patient) = 1246, 전체(Total) = 1383\n"
     ]
    }
   ],
   "source": [
    "print('환자경로{0}, 건강인 폴더{1}\\n'.format(patient_path,Healthy))\n",
    "print('건강인(Healthy) = {0},환자(Patient) = {1}, 전체(Total) = {2}'\\\n",
    "       .format(len(Path_healthy_image),len(Path_patient_image),len(path_total_fieldmap)))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pickle Load From Extracted Pickle file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(46, 46)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make Load,Save pickle function\n",
    "\n",
    "def save_pickle(fname, dict):\n",
    "    with open(fname,'wb') as fw:\n",
    "        pickle.dump(dict,fw)\n",
    "    fw.close()\n",
    "\n",
    "def load_pickle(fname):\n",
    "    with open(fname,'rb') as fr:\n",
    "        dict = pickle.load(fr)\n",
    "    return dict \n",
    "\n",
    "\n",
    "# 0번째 피클 파악\n",
    "\n",
    "pickle1= load_pickle(str(Path_patient_image[0]).split('label=')[0])\n",
    "pickle1.keys()\n",
    "# Keys Has 2 features s.t dict_keys(['Recurrence_plot', 'GASF'])\n",
    "\n",
    "pickle1['GASF'].shape\n",
    "# # 46*46\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# x = pickle1['GASF'][0]\n",
    "# y = pickle1['GASF'][1]\n",
    "# # plt.plot(pickle1['GASF'])\n",
    "\n",
    "# plt.mesh(x)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = str(Path_patient_image[0])\n",
    "           \n",
    "pickle_data= load_pickle(str(path.split('label=')[0]))\n",
    "\n",
    "# Extract Value\n",
    "Recur_plot,GASF = pickle_data['Recurrence_plot'],pickle_data['GASF']\n",
    "\n",
    "#R,T 이미지 적재 및 label 추출\n",
    "x = np.stack([Recur_plot,GASF])\n",
    "label = int(path.split('label=')[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(46, 46)\n",
      "(46, 46)\n",
      "(2, 46, 46)\n"
     ]
    }
   ],
   "source": [
    "print(Recur_plot.shape)\n",
    "print(GASF.shape)\n",
    "\n",
    "print(x.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make 2 Data, GASF _ RT & LabelRecur_plot\n",
    "Total_Recur = list()\n",
    "Total_GASF = list()\n",
    "\n",
    "Total_x=list()\n",
    "Total_label=list()\n",
    "\n",
    "\n",
    "for path in path_total_image:\n",
    "    # split[0] = path, split[1]= label\n",
    "    # 0번 경로는 label= 전은 패킷 경로, lael= 후는 label 정보\n",
    "    try:\n",
    "        pickle_data= load_pickle(str(path.split('label=')[0]))\n",
    "\n",
    "        # Extract Value\n",
    "        Recur_plot,GASF = pickle_data['Recurrence_plot'],pickle_data['GASF']\n",
    "\n",
    "        #R,T 이미지 적재 및 label 추출\n",
    "        x = np.stack([Recur_plot,GASF])\n",
    "        label = int(path.split('label=')[1])\n",
    "\n",
    "        #학습용 데이터로 활용\n",
    "        # append Each Value\n",
    "        Total_x.append(x)\n",
    "        Total_label.append(label)\n",
    "\n",
    "    except:\n",
    "        print(str(path[0].split('label=')[0]))\n",
    "\n",
    "T_label = [[x] for x in Total_label]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape = (1383, 2, 46, 46), Y shape =(1383, 1)\n",
      "Y Example = [[0], [0], [0], [0], [0], [0], [0], [0], [0], [0]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_keys(['features', 'labels'])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"X shape = {0}, Y shape ={1}\".format(np.array(Total_x).shape,np.array(T_label).shape))\n",
    "print('Y Example = {0}'.format(T_label[:10]))\n",
    "\n",
    "data_origin = {'features':Total_x,'labels':T_label}\n",
    "data_origin.keys()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Dataset, DataLoader & Shuffle, Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader,Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self,Dataset):\n",
    "        # self.Dataset = Dataset\n",
    "        self.features =  Dataset['features']\n",
    "        self.labels = Dataset['labels']\n",
    "\n",
    "        self.x = self.features\n",
    "        self.y = self.labels\n",
    "\n",
    "    # 생성자, 데이터를 전처리 하는 부분   \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    # 데이터셋의 총 길이를 반환하는 부분   \n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "\n",
    "        x = torch.FloatTensor(self.x[idx])\n",
    "        y = torch.FloatTensor(self.y[idx])\n",
    "        return x,y\n",
    "    # idx(인덱스)에 해당하는 입출력 데이터를 반환\n",
    "    \n",
    "    def __label__(self):\n",
    "        return self.labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wonsik.jung/anaconda3/envs/tenpy/lib/python3.9/site-packages/sklearn/utils/validation.py:856: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  array = np.asarray(array, order=order, dtype=dtype)\n",
      "/home/wonsik.jung/anaconda3/envs/tenpy/lib/python3.9/site-packages/sklearn/utils/validation.py:856: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  array = np.asarray(array, order=order, dtype=dtype)\n"
     ]
    }
   ],
   "source": [
    "dataset = CustomDataset(data_origin)\n",
    "seed = 1004\n",
    "batch_size = 8\n",
    "\n",
    "\n",
    "#               -Data split(train:0.7%(672), test:0.3*0.5=0.15%(144), validation:0.3*0.5=0.15%(145))-\n",
    "#               -Stratify에 설정으로, label 비율 유지( 정상인: 건강인 14%:86%)\n",
    "train_set, test_data0 = train_test_split(dataset, train_size = 0.7, test_size = 0.3, random_state = seed, stratify = dataset.labels)\n",
    "\n",
    "#               -strafify 옵션 설정을 위해 test_set0 생성후 label에 적용 추출\n",
    "\n",
    "test_set0 = CustomDataset({'features' : [x[:][0] for x in test_data0], 'labels': [x[:][1] for x in test_data0] })\n",
    "validation_set, test_set = train_test_split(test_set0, train_size = 0.5, test_size = 0.5, random_state = seed, stratify = test_set0.labels)\n",
    "\n",
    "#               -splited data를 Model에 사용하기 편하게 조절(splited데이터를 Dataloader로)-\n",
    "#               -From splited data To Dataloader-\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(train_set, batch_size = batch_size, shuffle = True)\n",
    "testloader = torch.utils.data.DataLoader(test_set, batch_size = batch_size, shuffle = False)\n",
    "validationloader = torch.utils.data.DataLoader(validation_set, batch_size = batch_size, shuffle = False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "968\n",
      "207\n",
      "208\n"
     ]
    }
   ],
   "source": [
    "print(len(train_set))\n",
    "print(len(validation_set))\n",
    "print(len(test_set))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Select & Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optimize\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import sklearn\n",
    "\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = models.efficientnet_b0(weights='EfficientNet_B0_Weights.IMAGENET1K_V1')\n",
    "\n",
    "model.features[0][0] = nn.Conv2d(in_channels = 2,out_channels = 32,\n",
    "                    kernel_size=3, stride =2, padding = 1)\n",
    "\n",
    "model.classifier = nn.Sequential(\n",
    "                nn.Dropout(p=0.2, inplace=True),\n",
    "                nn.Linear(in_features = 1280, out_features= 1, bias=True),\n",
    "                nn.Sigmoid()\n",
    "                )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = models.efficientnet_b0(weights='EfficientNet_B0_Weights.IMAGENET1K_V1')\n",
    "\n",
    "model.features[0][0] = nn.Conv2d(in_channels = 2,out_channels = 32,\n",
    "                    kernel_size=3, stride =2, padding = 1)\n",
    "\n",
    "model.classifier = nn.Sequential(\n",
    "                nn.Dropout(p=0.2, inplace=True),\n",
    "                nn.Linear(in_features = 1280, out_features= 1, bias=True),\n",
    "                nn.Sigmoid()\n",
    "                )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EfficientNet(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2dNormActivation(\n",
       "      (0): Conv2d(2, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): SiLU(inplace=True)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(32, 8, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(8, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (2): Conv2dNormActivation(\n",
       "            (0): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.0, mode=row)\n",
       "      )\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
       "            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(96, 4, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(4, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.0125, mode=row)\n",
       "      )\n",
       "      (1): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
       "            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.025, mode=row)\n",
       "      )\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(144, 144, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=144, bias=False)\n",
       "            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(144, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.037500000000000006, mode=row)\n",
       "      )\n",
       "      (1): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n",
       "            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.05, mode=row)\n",
       "      )\n",
       "    )\n",
       "    (4): Sequential(\n",
       "      (0): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(240, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=240, bias=False)\n",
       "            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.0625, mode=row)\n",
       "      )\n",
       "      (1): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
       "            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.07500000000000001, mode=row)\n",
       "      )\n",
       "      (2): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
       "            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.08750000000000001, mode=row)\n",
       "      )\n",
       "    )\n",
       "    (5): Sequential(\n",
       "      (0): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(480, 480, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=480, bias=False)\n",
       "            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.1, mode=row)\n",
       "      )\n",
       "      (1): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n",
       "            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.1125, mode=row)\n",
       "      )\n",
       "      (2): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n",
       "            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.125, mode=row)\n",
       "      )\n",
       "    )\n",
       "    (6): Sequential(\n",
       "      (0): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=672, bias=False)\n",
       "            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(672, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.1375, mode=row)\n",
       "      )\n",
       "      (1): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
       "            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.15000000000000002, mode=row)\n",
       "      )\n",
       "      (2): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
       "            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.1625, mode=row)\n",
       "      )\n",
       "      (3): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
       "            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.17500000000000002, mode=row)\n",
       "      )\n",
       "    )\n",
       "    (7): Sequential(\n",
       "      (0): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152, bias=False)\n",
       "            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(1152, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.1875, mode=row)\n",
       "      )\n",
       "    )\n",
       "    (8): Conv2dNormActivation(\n",
       "      (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): SiLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.2, inplace=True)\n",
       "    (1): Linear(in_features=1280, out_features=1, bias=True)\n",
       "    (2): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define (Traing, Test) And Run Epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import time\n",
    "import sklearn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "model=model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(),lr = 1e-3)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "save_path=\"//home//wonsik.jung//Vscode test//GAF&Recur_DL_Model\"\n",
    "model_save_path = str(save_path)+\"/savedmodel.pth\"\n",
    "\n",
    "train_writer = SummaryWriter(str(save_path)+'//Loss//Train', comment = 'Train')\n",
    "validation_writer = SummaryWriter(str(save_path)+'//Loss//Validation', comment = 'Validation')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#                            --------(3-1)Define Train 정의-------\n",
    "def train(trainloader, model, optimizer, device,trainwriter=None, epoch: Optional[int] =None):\n",
    "    model.train()\n",
    "    with tqdm(total = len(trainloader), desc = 'Training Processing') as pbar:\n",
    "        for batch_idx,(data, label) in enumerate(trainloader):\n",
    "            data, label = data.to(device), label.to(device)\n",
    "            # Set optimizer_Gradient()\n",
    "            # Set Forward + Backward + Optimize\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output,label)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            pbar.update()\n",
    "\n",
    "        if trainwriter != None:\n",
    "            train_writer.add_scalar('Train_Loss', loss,epoch+1)\n",
    "\n",
    "\n",
    "\n",
    "#                            --------(3-2)Define validation,test 정의-------\n",
    "def val_test(dataloader, model, device, val_testwriter=None, epoch: Optional[int] =None):\n",
    "    model.eval()\n",
    "    loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, label) in enumerate(dataloader):\n",
    "            data, label = data.to(device), label.to(device)\n",
    "            output = model(data)\n",
    "            pred = torch.round(output)\n",
    "            loss += criterion(output, label).item()\n",
    "            #   -------- BinaryClass므로 round를 넣어 0또는 1로 결과 예측----\n",
    "            #   -------- pred_result와 label을 batch 마다 적재하는 과정-----\n",
    "            label_cpu, pred_cpu, output_cpu = label.detach().cpu().numpy(), pred.detach().cpu().numpy(), output.detach().cpu().numpy()\n",
    "\n",
    "            if batch_idx == 0:\n",
    "                label_result, pred_result, proba_result = label_cpu, pred_cpu, output_cpu\n",
    "            else:\n",
    "                label_result, pred_result, proba_result = np.concatenate([label_result, label_cpu]), np.concatenate([pred_result, pred_cpu]), np.concatenate([proba_result, output_cpu])\n",
    "\n",
    "        if val_testwriter != None:\n",
    "            val_testwriter.add_scalar('validation_Loss', loss,epoch+1)\n",
    "            val_testwriter.add_graph(model,data)\n",
    "\n",
    "\n",
    "    return loss, label_result, pred_result, proba_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_epoch(model, optimizer, device, trainloader, validationloader, \n",
    "              epochs: int, patience: Optional[int]=4, model_save_path: Optional[str]=None, \n",
    "              train_writer=None, val_testwriter=None, scheduler=None):\n",
    "    cnt = 0\n",
    "    for epoch in range(epochs):\n",
    "        ###       ------Train Proecess\n",
    "        train(trainloader, model, optimizer, device, trainwriter=train_writer, epoch=epoch)\n",
    "\n",
    "        ###       -----Validation Process\n",
    "        val_loss, label_result, pred_result, proba_result = val_test(validationloader, model, device, val_testwriter=validation_writer, epoch=epoch)\n",
    " \n",
    "        # validation_writer.add_scalar('Validaion_loss', validation_loss,epoch+1)\n",
    "            \n",
    "\n",
    "        ###   -------- Early Stop & Model Save\n",
    "        try:\n",
    "            if min_loss == None:\n",
    "                min_loss = val_loss\n",
    "        except:\n",
    "            min_loss = val_loss\n",
    "\n",
    "        if val_loss > min_loss:\n",
    "            cnt += 1\n",
    "            if cnt >= patience:\n",
    "                break\n",
    "\n",
    "        else:\n",
    "            try:\n",
    "                min_loss = np.min(min_loss,val_loss)\n",
    "                cnt = 0\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "    ###  ------ scheduler Apply\n",
    "    if scheduler!= None:\n",
    "        scheduler.step()\n",
    "\n",
    "\n",
    "    ###     ----if epoch Done!, save model state_dict\n",
    "    if model_save_path!= None:\n",
    "        torch.save(model.state_dict(),model_save_path)\n",
    "\n",
    "\n",
    "    acc = sklearn.metrics.accuracy_score(label_result,pred_result)\n",
    "    print('\\nFinal Epoch:{} 평균 loss: {:.9f}, 정확도: {:.3f})\\n'.format(epoch+1,val_loss,acc))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Processing: 100%|██████████| 121/121 [00:16<00:00,  7.40it/s]\n",
      "Training Processing: 100%|██████████| 121/121 [00:12<00:00,  9.48it/s]\n",
      "Training Processing: 100%|██████████| 121/121 [00:05<00:00, 23.40it/s]\n",
      "Training Processing: 100%|██████████| 121/121 [00:05<00:00, 23.26it/s]\n",
      "Training Processing: 100%|██████████| 121/121 [00:05<00:00, 23.22it/s]\n",
      "Training Processing: 100%|██████████| 121/121 [00:05<00:00, 22.82it/s]\n",
      "Training Processing: 100%|██████████| 121/121 [00:05<00:00, 22.83it/s]\n",
      "Training Processing: 100%|██████████| 121/121 [00:05<00:00, 22.72it/s]\n",
      "Training Processing: 100%|██████████| 121/121 [00:07<00:00, 16.62it/s]\n",
      "Training Processing: 100%|██████████| 121/121 [00:14<00:00,  8.26it/s]\n",
      "Training Processing: 100%|██████████| 121/121 [00:14<00:00,  8.20it/s]\n",
      "Training Processing: 100%|██████████| 121/121 [00:14<00:00,  8.22it/s]\n",
      "Training Processing: 100%|██████████| 121/121 [00:14<00:00,  8.27it/s]\n",
      "Training Processing: 100%|██████████| 121/121 [00:11<00:00, 10.16it/s]\n",
      "Training Processing: 100%|██████████| 121/121 [00:06<00:00, 17.83it/s]\n",
      "Training Processing: 100%|██████████| 121/121 [00:06<00:00, 17.67it/s]\n",
      "Training Processing: 100%|██████████| 121/121 [00:06<00:00, 17.91it/s]\n",
      "Training Processing: 100%|██████████| 121/121 [00:06<00:00, 17.49it/s]\n",
      "Training Processing: 100%|██████████| 121/121 [00:10<00:00, 11.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Epoch:19 평균 loss: 9.733301514, 정확도: 0.903)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "epochs= 100\n",
    "\n",
    "# validationloader, train_writer, val_testwriter, epoch\n",
    "\n",
    "\n",
    "# run_epoch(model, optimizer, device, trainloader, validationloader, epochs,patience=5, model_save_path=model_save_path, train_writer=train_writer, val_testwriter=validation_writer)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Saved Model Perfomance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도:0.883, ROC:0.680,정밀도:0.909, 민감도:0.960, F1 점수:0.934\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(save_path+'/savedmodel.pth'))\n",
    "                                 \n",
    "label_result, pred_result = test(testloader,model,device)\n",
    "\n",
    "acc = sklearn.metrics.accuracy_score(label_result,pred_result)\n",
    "roc = sklearn.metrics.roc_auc_score(label_result,pred_result)\n",
    "pre = sklearn.metrics.precision_score(label_result,pred_result)\n",
    "sen = sklearn.metrics.recall_score(label_result,pred_result)\n",
    "f1 = sklearn.metrics.f1_score(label_result,pred_result)\n",
    "\n",
    "print('정확도:{:.3}, ROC:{:.3f},정밀도:{:.3f}, 민감도:{:.3f}, F1 점수:{:.3f}'.format(acc,roc,pre,sen,f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(145, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([132.], dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(pred_result.shape)\n",
    "sum(pred_result)\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performance_score(label,predict):\n",
    "    '''\n",
    "    Input\n",
    "    label = model.label\n",
    "    proba = model.predict_proba(test)\n",
    "    \n",
    "    Output\n",
    "\n",
    "    accuracy = accuracy_score(y_test, pred)\n",
    "    precision = precision_score(y_test, pred)\n",
    "    recall = recall_score(y_test, pred)  # sensitivity\n",
    "    f1 = f1_score(y_test, pred)\n",
    "    '''\n",
    "\n",
    "    #####from confusion matrix calculate accuracy\n",
    "    CM = sklearn.metrics.confusion_matrix(label,predict)\n",
    "    TN, FP, FN, TP = CM[0,0], CM[0,1], CM[1,0], CM[1,1]\n",
    "\n",
    "    specificity = TN/(FP+TN)\n",
    "\n",
    "    precision = TP/(TP+FP)\n",
    "    recall = TP/(TP+FN) # sensitivity\n",
    "\n",
    "    accuracy=(TP+TN)/(TN+FP+FN+TP)\n",
    "    F1 = 2*(precision*recall)/(precision+recall)\n",
    "\n",
    "    return F1, accuracy, precision, recall, specificity\n",
    "\n",
    "\n",
    "\n",
    "def opt_threshold(label: np.array, proba: np.array) -> list:\n",
    "    '''\n",
    "    Input\n",
    "\n",
    "    label = model.label\n",
    "    proba = model.predict_proba(test)\n",
    "\n",
    "    Output\n",
    "\n",
    "    roc = pd.DataFrame({'fpr' : pd.Series(fpr, index=i),'tpr' : pd.Series(tpr, index = i),\n",
    "    '1-fpr' : pd.Series(1-fpr, index = i), 'tf' : pd.Series(tpr - (1-fpr), index = i),\n",
    "    'thresholds' : pd.Series(thresholds, index = i)})\n",
    "    '''\n",
    "\n",
    "    fpr, tpr, thresholds = sklearn.metrics.roc_curve(label, proba)\n",
    "\n",
    "    i = np.arange(len(tpr)) # index for df\n",
    "    roc_info = pd.DataFrame([fpr, tpr, 1-fpr, tpr-(1-fpr), thresholds], index=['fpr','tpr','1-fpr','tf','thresholds']).T\n",
    "    cutoff_info = roc_info.iloc[(roc_info.tf).abs().argsort()[:1]]\n",
    "    \n",
    "    return np.round(cutoff_info['thresholds'].tolist()[0],4)\n",
    "\n",
    "def evaluate_prediction(label, proba, cutoff=1):\n",
    "    '''  \n",
    "    Input\n",
    "\n",
    "    label = model.label\n",
    "    proba = model.predict_proba(test)\n",
    "    \n",
    "    Output\n",
    "\n",
    "     accuracy = accuracy_score(y_test, pred)\n",
    "     precision = precision_score(y_test, pred)\n",
    "     recall = recall_score(y_test, pred)  # sensitivity\n",
    "     f1 = f1_score(y_test, pred)\n",
    "     '''\n",
    "\n",
    "    \n",
    "    if cutoff==1:\n",
    "        thr = opt_threshold(label,proba)\n",
    "        pred = proba.copy()\n",
    "        pred[pred<thr] = 0\n",
    "        pred[pred>=thr] = 1\n",
    "    else:\n",
    "        thr = cutoff\n",
    "        pred = proba.copy()\n",
    "        pred[pred<thr] = 0\n",
    "        pred[pred>=thr] = 1\n",
    "\n",
    "\n",
    "    #####from confusion matrix calculate accuracy\n",
    "    CM = sklearn.metrics.confusion_matrix(label,pred)\n",
    "    TN, FP, FN, TP = CM[0,0], CM[0,1], CM[1,0], CM[1,1]\n",
    "\n",
    "    specificity = TN/(FP+TN)\n",
    "    precision = TP/(TP+FP)\n",
    "    recall = TP/(TP+FN) # sensitivity\n",
    "    accuracy=(TP+TN)/(TN+FP+FN+TP)\n",
    "    F1 = 2*(precision*recall)/(precision+recall)\n",
    "\n",
    "    # fpr, tpr, _ = roc_curve(label, model.predict_proba(test)[:,1])\n",
    "    auc = sklearn.metrics.roc_auc_score(label, proba)\n",
    "\n",
    "    return F1, accuracy, auc, precision, recall, specificity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1:0.953, Acc:0.918, Auc:0.923, Precision:0.989, Recall:0.920, Specificity:0.905\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'F1:0.953, Acc:0.918, Auc:0.923, Precision:0.989, Recall:0.920, Specificity:0.905'"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(model_save_path))\n",
    "\n",
    "_, test_label, test_predict, test_proba = val_test(testloader, model, device)\n",
    "\n",
    "\n",
    "# F1, accuracy, precision, recall, specificity = performance_score(test_label,test_predict)\n",
    "Cut = opt_threshold(test_label, test_proba)\n",
    "Result = evaluate_prediction(test_label, test_proba, cutoff = Cut)\n",
    "\n",
    "\n",
    "# return F1, accuracy, auc, precision, recall, specificity\n",
    "print('F1:{0:.3f}, Acc:{1:.3f}, Auc:{2:.3f}, Precision:{3:.3f}, Recall:{4:.3f}, Specificity:{5:.3f}'\n",
    "      .format(Result[0], Result[1], Result[2], Result[3], Result[4], Result[5]))\n",
    "\n",
    "\n",
    "#efficiet0.V1\n",
    "'F1:0.953, Acc:0.918, Auc:0.923, Precision:0.989, Recall:0.920, Specificity:0.905'\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tenpy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "40bc2da231397d3a090bcc74bf63cff42ce20ec6937355d145d70da3112071b1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
